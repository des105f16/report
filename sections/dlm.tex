% !TEX root = ../master.tex

\newcommand{\labelof}[1]{\underline{#1}}
\newcommand{\dlmactsfor}{\dlmc{if\_acts\_for}}
\newcommand{\dlmdeclassify}{\dlmc{declassify}}
\newcommand{\dlmpc}{$\underline{pc}$}
\newcommand{\mathcomment}[1]{\color{green!50!black}{#1}}
\newcommand{\ceil}[1]{\lceil#1\rceil}

\chapter{The Decentralized Label Model}\label{dlm}
The Decentralized Label Model \cite{myers1997, myers1998, myers2000} is a model for ensuring information flow control in a system.
This is done by annotating source code with security policies, in the form of labels attached to data-holding constructs.
This chapter will present the necessary information needed to understand the implementation presented in \cref{ctif}.
The descriptions and definitions in this section are based on \cite{myers1997, myers1998, myers2000}.
Throughout the examples, we will use the same grammar as that used by our implementation.

In this presentation of DLM some details have been left out.
These details are related to run-time concepts, such as \emph{principal hierarchy} and \emph{authority}.
Our focus will be on the concepts needed for compile-time checking.
The impliciations of this limitation will be further discussed in \cref{conclusion}.

Finally, in \cref{dlm:constraints,dlm:inferring_labels} we will present our own formalizations related to the concept of \emph{label inference}, which is briefly introduced in \cite{myers1997}.
We will use these formalizations as a base in order to define the label inference algorithm, which is an essential part of our implementation.

\section{Labels and policies}\label{dlm:policies}
Throughout a program, values are declared, initialized, and assigned to variables and other value-holders.
Value-holders are collectively known as \emph{slots}, which cover constructs such as variables, structs, and other storage locations.
In order to ensure that a certain assignment is legal, such that no information is unintentionally leaked, we assign \emph{labels} to slots so that their ``security'' can be compared.
This way we can ensure that an assignment is only legal in the cases where higher-security values aren't assigned to lower-security slots.
In \cite{myers1997} labels are seen as an extension to types, and would therefore be applicable anywhere a type normally would be applicable.

Each slot is associated with a label, that describes how the data in the slot can be handled.
We denote the label of slot $s$ as $\underline{s}$.
Referring to labels in this fashion serves the same purpose as letters in algebra; it allows for handling labels without knowledge about their actual value.

For a given label it is possible to define both read (\emph{privacy}) and write (\emph{integrity}) policies.
The first represents information flow out of a system and the latter information flow into a system.
In this report only read policies will be considered.
Due to the relation between the two types of policies, we note that write policies can be ensured in a fashion similar to read policies.
Because of this we choose to simplify the scope of the report such that it only discusses read policies.

\myparagraph{Principals}
In the following, we employ the concept of \emph{principals}.
A principal (or \emph{subject}, \emph{agent}) is an entity in a system that represents some interest.
In short, principals represent real-world users or the authority under which a program/system runs.

In the following we make use of the special set of principals; $*$.
This set contains all the principals in a system.

\myparagraph{Labels}
A label can be described as a set of policies, where a policy consists of an owner principal $o$ and a set of reader principals $r_1, r_2, \dots, r_n$.
Formally we provide the following, equivalent definition:

\begin{definition}{Labels}\label{dlm:def:label}
A label $L$ is a set of owners $owners(L)$, and a function $readers(L, o)$ that retrieves the set of reader principals that $o$ allows to read.
Note that given this definition we have that $$o \notin owners(L) \Rightarrow readers(L, o) = *$$
\end{definition}

Owners are allowed to change their own policies within a label.
We describe how this is done in \cref{dlm:auth_and_declass}.
If an owner is not part of its own policies reader set, that owner is not allowed to read from the slot associated with the label.
He is however still allowed to change his policy.

\myparagraph{The effective reader set}
To ensure that the policies of all owners in a label are enforced, only readers that all owners ``agree'' can read from the slot associated with the label.
This is known as the \emph{effective reader set}, which is the intersection of \emph{all} reader sets of a label:

\begin{definition}{The effective reader set}\label{dlm:def:effectivereaders}
  The reader set of a label $L$ is the set of readers that are in the reader set of all owners:
  $$readers(L) = \bigcap_{o \in *} readers(L, o)$$
\end{definition}

\begin{example}{A label with two policies}\label{dlm:ex:simple_label}
  Below is an example of a label with two policies using the notation from above;
  $$L_1 = \{o1 \rightarrow r1, r2; o2 \rightarrow r2, r3\}$$

  The owner set of the label is $owners(L_1) = \{o1, o2\}$, and its reader sets are $readers(L_1, o1) = \{r1, r2\}, readers(L_1, o2) = \{r2, r3\}$.
  By intersecting the reader sets of the label we get the effective reader set.
  In practice we can disregard the reader set of labels that are not owners.
  Thus the effective reader set is
  $$readers(L_1) = \{r1,r2\} \cap \{r2, r3\} = \{r2\}$$
\end{example}

\section{Security class lattice}
A set of DLM labels can be seen as a \emph{security class lattice} \cite[pp. 6-7]{myers1997}.
A lattice is a partially ordered set, for which each pair of elements in the set have a unique least upper bound (the join/$\sqcup$ of the labels) and a greatest lower bound (the meet/$\sqcap$ of the labels).
In the case of DLM, the lattice is the set of all labels and it is ordered by how restrictive the labels are.

A label without owners effectively has no restrictions on the data it protects.
As such it is the lower bound $\bot$ of the lattice.
Likewise the label that has all owners in a system (as defined in \cref{dlm:def:label}), but no readers allowed by either of the owners, is the most restrictive label.
This is the upper bound $\top$ of the lattice.

With these we have a bounded lattice as illustrated by \cref{dlm:lattice_fig}.
In the following sections we define the properties of this security lattice.
These will match the description of the lattice bounds given above.

\begin{figure}
  \begin{center}
  \begin{tikzpicture}[inner sep=1mm, >=latex]
    \node (l1) at (-1,0) {$L_1$};
    \node (l2) at (1,0) {$L_2$};

    \node (meet) at (0,-1) {\small $L_1$ {\LARGE $\! \sqcap \!$} $L_2$};
    \node (bot) at (0,-2) {$\bot$};

    \node (join) at (0,1) {\small $L_1$ {\LARGE $\! \sqcup \!$} $L_2$};
    \node (top) at (0,2) {$\top$};

    \draw[->] (l1) edge (join);
    \draw[->] (l2) edge (join);
    \draw[->] (l1) edge (meet);
    \draw[->] (l2) edge (meet);

    \draw[->, dashed, gray] (l1) edge [bend left] (top);
    \draw[->, dashed, gray] (join) edge (top);
    \draw[->, dashed, gray] (l2) edge [bend right] (top);

    \draw[->, dashed, gray] (l1) edge [bend right] (bot);
    \draw[->, dashed, gray] (meet) edge (bot);
    \draw[->, dashed, gray] (l2) edge [bend left] (bot);
  \end{tikzpicture}
  \caption{Abstraction of security lattice}
  \label{dlm:lattice_fig}
  \end{center}
\end{figure}

\subsection{Composite labels}
We can construct composite labels by either joining or meeting two labels.
A composite label $L_1 \sqcup L_2$ or $L_1 \sqcap L_2$ represents nodes in a composite label structure.
To employ these labels similarly to the previously defined labels, we define the owner and reader sets of the composite labels:

\begin{definition}{Label Join}\label{dlm:def:join}
The join operation is denoted by the operator $\sqcup$ and represents the least restrictive label that is as restrictive as either operand:
  \begin{align*}
    owners(L_1 \sqcup L_2) &= owners(L_1) \cup owners(L_2) \\
    readers(L_1 \sqcup L_2, o) &= readers(L_1, o) \cap readers(L_2, o)
  \end{align*}
\end{definition}
\begin{definition}{Label Meet}\label{dlm:def:meet}
The meet operation is denoted by the operator $\sqcap$ and represents the most restrictive label that is no more restrictive than either operand:
  \begin{align*}
    owners(L_1 \sqcap L_2) &= owners(L_1) \cap owners(L_2) \\
    readers(L_1 \sqcap L_2, o) &= readers(L_1, o) \cup readers(L_2, o)
  \end{align*}
\end{definition}

Label composition is an integral part of DLM.
Label joins allows for label evaluation of expressions, as described in \cref{extraction} and provides the means for representing implicit flows via scope blocks, as described in \cref{dlm:implicit_flows}.
Additionally when inferring labels, the meet operation allows us to infer the most restrictive label that agrees with the preexisting labels.

\subsection{Label comparison}\label{dlm:comparison}
As mentioned above, labels provide restrictions on how data can flow to and from slots that the labels ``protect''.
In order to check that security policies are enforced throughout a program, we need to be able to compare how restrictive labels are.
To do this we introduce the ``at most as restrictive as''-relation ($\sqsubseteq$) from \cite{myers1997}.

\begin{definition}{Label Restrictiveness}\label{dlm:def:restrict}
  Label $L_1$ is at most as restrictive as label $L_2$ ($L_1 \sqsubseteq L_2$) if it (\ref{dlm:def:restrict:owners}) does not have more owners and (\ref{dlm:def:restrict:readers}) each owner does not have a smaller reader set:
  \begin{align}
    & owners(L_1) \subseteq owners(L_2) \text{ and} \label{dlm:def:restrict:owners} \\
    & \forall o \in owners(L_1) . readers(L_1, o) \supseteq readers(L_2, o) \label{dlm:def:restrict:readers}
  \end{align}
\end{definition}

We can employ this definition to determine if two labels are equal:
If $L_1 \sqsubseteq L_2$ and $L_2 \sqsubseteq L_1$ then we must have that $L_1 = L_2$.
Additionally we note that given \cref{dlm:def:join,dlm:def:restrict} we note that $L_1 \sqcup L_2 \sqsubseteq L_3$ is equivalent to $L_1 \sqsubseteq L_3 \wedge L_2 \sqsubseteq L_3$.
This provides a simplified means for comparing composite label-structures.

\section{Channels}\label{dlm:channels}
Throughout a system data can enter and leave, before- and afterwhich labels cannot be enforced.
The entry points for data entering and leaving can therefore be seen as crucial endpoints, where we want to do additional checking to ensure we are not unintentionally leaking.
This is done through \emph{channels}, specifically \emph{input channels} and \emph{output channels}.
However, input channels are not special and simply apply the label of the channel to the inputted data.

\myparagraph{Output channels}
On the other hand, output channels are especially crucial in that it is an endpoint where we will intentionally ``leak'' data, and afterwhich we are not able to control the further flow of this data.
Intuitively, an output channel represents data going outside the system, e.g. through a physical printer, console output, or web request.

Output channels therefore differ in the way that the ``security'' is compared.
Output channels need only declare a list of readers, that represent the principals that have access to the endpoint.
We then need to compare this reader list to the effective reader set of the label for the data to be outputted.
Any reader declared by the output channel must be in the effective reader set for the label of the outputted data, to ensure that we do not output data to a channel where the principals aren't allowed to read it.

\section{Implicit flows}\label{dlm:implicit_flows}
When assigning a value to a slot, possibly from another slot, it is an explicit flow.
In addition to explicit flows, it is also possible to have implicit flows throughout a program, due to conditional control structures (such as loops).
For whatever assignments we would do inside a block (or within a deeper block hierarchy) we need to take the predicates of these blocks into consideration.
This is done by adding the concept of \emph{program counter labels}, denoted \dlmpc.
For each scope we will have an implicit \dlmpc, depending on the surrounding predicates' labels.

\begin{example}{Implicit leak}\label{dlm:ex:implicit_leak}
  Consider the following program (each line commented with the current \dlmpc):
  \begin{lstlisting}[style=dlmc]
int {{a->z,y}} val = 0;   // $\mathcomment{\bot}$
int {{a->y}} cond = 1;    // $\mathcomment{\bot}$
if (cond) {
  val++;                  // $\mathcomment{\bot \sqcup \{a \rightarrow y\}}$
}
return val;               // $\mathcomment{\bot}$
\end{lstlisting}
  In this example we would have an implicit leak of \dlmc{cond}, by way of \dlmc{val}.
  This is due to the assignment of \dlmc{val} inside of the \dlmc{if}-statement, as \dlmpc~ is increased to the join of the outer scope ($\bot$) and the label of the scope's predicate (\labelof{\dlmc{cond}}).
  For this program to pass \thetool, we would have to add a more strict policy to \dlmc{val}, so that it could match the policy of \dlmc{cond}.
\end{example}

\section{Authority and declassification}\label{dlm:auth_and_declass}
In order to avoid mishaps by setting certain policies too lax, so that certain flows are permitted, we can temporarily set some policies to be more strict, and then only relax (\emph{declassify}) them when we really need to.
In order to do this, the concept of \emph{authority} is introduced.

At any point during execution of a program, there will be a \emph{true authority}, which is the maximal authority by which the program can carry out operations.
Whenever we call a function, we can do this with a certain authority, corresponding to running that method with the authority of a specific principal or principals.
However, in order to take advantage of this given authority, we need to explicitly claim it.
This is done by using the \emph{if acts for} construction\footnote{We use the syntax: \dlmc{-->? p1, p2 \{ /*statements*/ \}} -- more about this in \cref{ctif:informal:ifactsfor_declassify}).}, which takes as an argument one or more principals for which we want to obtain authority to carry out statements in the block.
Only after calling this function will we obtain the \emph{effective authority} to act for the principals, enabling us to carry out the statements within the  block that would normally only be permissible for the principals themselves.

If the check fails, the statements block is not executed.
If the check passed, the statements will be executed under the the newly established authority, e.g. for a principal $p$.
With $p$ in the effective authority we can perform declassification -- deliberately and explicitly relaxing the security policies in which $p$ is owner.
This is done by using the \emph{declassify} construction\footnote{We use the syntax: \dlmc{<|v, l|>} -- more about this in \cref{ctif:informal:ifactsfor_declassify}).}, with a slot $v$ and a new label $l$ as arguments, returning the value $v$ relabeled to $l$.
The relabeling by declassification rule (the inference rule) is defined as follows:

\begin{definition}{Relabeling by declassification}
  Let $P$ denote the set of principals in the current authority, then
  \[
  \frac
  {
    \splitfrac
    {
      L_A = \mathlarger\sqcupl_{p \in P} \{p \rightarrow \emptyset \}
    }
    {
      L_1 \sqsubseteq L_2 \sqcup L_A
    }
  }
  {
    L_1 \text{ may be declassified to } L_2
  }
  \]
\end{definition}

The combination of these two concepts, \emph{if acts for} and \emph{declassify}, is especially useful when we have sensitive inputs to a method and want to carry out our calculations without the fear of neither explicit or implicit leakage.
This way we can keep our strict policies throughout the calculations of the method and only relax the label once we want to return the result.

\begin{example}{Temporarily restricting a label}
  Building on \cref{dlm:ex:implicit_leak}, we can set the label for \dlmc{val} to match that of \dlmc{cond}, and then only relax that label when we need to return \dlmc{val}, ensuring that we have the ability to do so:
  \begin{lstlisting}[style=dlmc]
int {{a->y}} val = 0;
int {{a->y}} cond = 1;
if (cond) {
  val++;
}
this -->? a {
  return <|x, {{a->y,z}}|>;
}
return -1;
\end{lstlisting}
  While we still leak some information about \dlmc{cond}, we now do it explicitly.
\end{example}

Worth noting in the above example is the use of the special keyword \dlmc{this} in the if-acts-for statement.
This is a run-time concept that signifies from where the specified authority (here \dlmc{a}) should be obtained from.
When the \dlmc{this} keyword is used, it refers to an implicit principal that represents the function itself.
Alternatively, if instead the \dlmc{caller} keyword was stated, the specified authority is compared with an explicit authority declared for each call to the function.
Using our syntax, such a function call would look like:
\begin{center}
  \dlmc{foo<<<a>>>();}
\end{center}

\section{Label constraints}\label{dlm:constraints}
Using the concept of labels as described in the above sections, code can be annotated to express what type of information flow is permissible.
By examining all points in code where information flows it can then be determined if these information flows are valid.
The validity of an information flow is expressed as a constraint on the form $L_1 \sqsubseteq L_2$, as described in \cref{dlm:def:restrict}.

When information flows from one point $A$ to another $B$, it is required that $B$ is allowed to read the same information as $A$.
Thus we can, in general, represent such an information flow as $\labelof{A} \sqsubseteq \labelof{B}$.
This represents that access to the information known at point $A$ is no more restrictive than access to that of point $B$.
An example of such an information flow is the assignment of a value to a variable:

$$var := expression$$

From this simple example it is clear that information flows from the expression to the variable and thus a constraint for this example would be $\labelof{expression} \sqsubseteq \labelof{var}$.
Though as we described in \cref{dlm:implicit_flows} information can also flow implicitly, and because of that we must include all available information in the flow constraint.
The currently available information is represented as the program counter $\labelof{pc}$ and must be included on the left side of the constraint, to produce:

$$\labelof{expression} \sqcup \labelof{pc} \sqsubseteq \labelof{var}$$

This expansion of constraints captures that information assigned to $var$ could contain information about the values on which the current program counter is based.
Implicit flows will thus be considered similarly to any other constraints.

\myparagraph{Extracting constraints}
The above example illustrates the concept of information flow as described by a constraint.
The information flow of a program can be describe by a number of constraints.
Checking that each constraint is valid using \cref{dlm:def:restrict} will ensure that the information flow of the entire program is valid, given the declared labels.
In \cite{myers1997, myers1998, myers2000} an informal description of which constraints a program is associated with is provided.
In \cref{extraction} we provide a formalized approach to extracting constraints from label declarations.

Below is a simple example of the how labels and constraints are extracted from source code.
The example does not consider some of the complexities that arise from checking a complete function (or set of functions), but merely serves to provide a simple example of what checking constraints entails.

\begin{example}{Declaration and assignment}\label{dlm:ex:extract}
  Two variable declarations with associated labels and an assignment operation.

  \begin{lstlisting}[style=dlmc]
int {{a->y, x}} val = 5;  // $\mathcomment{\{a \rightarrow y, x\}}$, $\mathcomment{\labelof{5} \sqsubseteq \labelof{val}}$
int {{a->y}} res;         // $\mathcomment{\{a \rightarrow y\}}$

res = val;                // $\mathcomment{\labelof{val} \sqsubseteq \labelof{res}}$
\end{lstlisting}

  As described above, the assignment operation introduces a constraint that must be checked for the information flow to be valid.
  Additionally the declaration of the \dlmc{val} variable includes an assignment operation.
  Because of this we have an additional, yet trivial, constraint for the declaration.
  Thus we have the following constraints for the above code and know that the information flow in the code is valid.
  $$\labelof{5} \sqsubseteq \labelof{val} \equiv \bot \sqsubseteq \{a \rightarrow y, x\}$$
  $$\labelof{val} \sqsubseteq \labelof{res} \equiv \{a \rightarrow y, x\} \sqsubseteq \{a \rightarrow y\}$$
\end{example}

% Introduce constraints and what information they represent.
% Reference the previous examples in the chapter.
% Provide reference to the semantics chapter for "how to retrieve constraints".

\section{Inferring labels}\label{dlm:inferring_labels}
The components of constraints are labels.
Some of these labels will be constructed from the evaluation of expressions, as is the example with the assignment above.
But the values of the labels themselves all stem from a declaration somewhere.
Expressions merely employ the label associated with the individual slot.
In \cref{dlm:policies} we described how slots and functions are associated with labels at declaration.
Using these declared labels we are able to construct constraints that can then be checked to see if a programs information flow is valid, as exemplified by \cref{dlm:ex:extract}.

As constraints use the \textit{label of} construct to represent slots, it is required that all slots be associated with a label.
In other words; the programmer must determine information flow for each slot within a program.
On the surface this is a great feature of DLM, as it forces the programmer to actively consider information flow throughout his application.
On the downside, the programmer is also forced to spend time considering the information flow of every variable in a function.
Some of these might be trivially associated with other variables and thus simple to label.
But as a rule of thumb anything that is simple and trivial to do should be automated, freeing the programmer to handle more complex tasks.
Having to label every slot will also increase the complexity of maintaining code, as a single change in a label might have to propagate many places.

\myparagraph{Inference in DLM}
To address the above concerns, \cite{myers1997} introduces the concept of label inference.
Inferring labels allows a program, such as a specialized compiler, to compute appropriate labels for unlabeled slots.
The inferred labels are ``reverse engineered'' based on the constraints of a program.

The means of inference is described informally in \cite{myers1997}; both in terms of the required structures and the algorithm.
Because the description is only provided informally there are some uncertainties for inference.
First and foremost, how constraints are extracted from code, as has already been discussed.
But additionally which label declarations can be omitted and what the meaning of such an omission is.

In our approach we allow any label declaration to be omitted, and in \cref{extraction} we provide a formal definition for the meaning of each omitted label as well as a definition for how constraints are extracted.
In the following we formalize the data structure (\cref{dlm:inf:types}) and algorithm (\cref{dlm:inf:algsection}) described in \cite{myers1997} such that they can be applied to the constraints that are extracted.

\subsection{Label types}\label{dlm:inf:types}
To manage any unlabeled entities we introduce special label types that have no actual principal policies.
Instead these labels function as placeholders for policies.
As such, we will need to employ a special type of handling for the $\sqsubseteq$, $\sqcup$ and $\sqcap$ operations for these new labels.

Below we introduce two new label types; variable and constant labels and define how we can apply each of the above three operations to the new label types.

\subsubsection{Variable labels}
A \emph{variable label} represents a label that should be computed through inference.
When applying inference the label is associated with another label value, known as its current upper bound.
This upper bound will initially have the value $\top$, representing the most restrictive label and will be relaxed as the algorithm progresses.

When the algorithm completes, the current upper bound of a variable label represents the most restrictive label that could be employed instead of the variable label.
This effectively represents the inferred label.

\subsubsection{Constant labels}
A \textit{constant label} represents a label that is not associated with a set of policies, and one that should \textit{not} be computed through inference.
Instead constant labels are used to implement label polymorphism.
Specifically this will be used to provide polymorphism for function parameters and arguments.
The label of a function parameter can be represented using a constant label and will be replaced by the label of the argument when the function is called.

Because of the current upper bound of variable labels, a constant label can be propagated throughout a set of constraints.
This allows variables in the function and the return label of the function to contain the constant label.

\subsubsection{Joins and meets}
As the two new label types are not directly associated with policies, the join or meet of them is only represented as a composite structure.
This is true regardless of the label they are being joined or met with.

Our interest in these labels is only temporary.
Either to determine the most restrictive upper bound or for the purpose of label polymorphism.
Because of this we are content using a composite representation.
When replacing a variable or constant label with an actual set of policies the composite structure will be removed by virtue of \cref{dlm:def:join,dlm:def:meet}.

\subsubsection{Label restriction}
For the inference algorithm to function using the new label types it is required that we can determine if one label is no more restrictive than another label.
Thus we must determine how to handle this comparison for both variable and constant labels.
Additionally we must determine the comparison for both join and meet composite labels.

Firstly we note that a definition for the comparison is not required for variable labels.
In the algorithm we employ the current upper bound of variable labels for the label comparison.
Because of this we will never directly compare variable labels with other labels.

For any constant label $L_c$ we define that $L_c \not \sqsubseteq L$ and $L \not \sqsubseteq L_c$ for any label $L$, with the natural exception of $L_c = L$.
For label joins we merely seek to identify those cases where we are sure that the comparison evaluates to true, and let the remaining evaluate to false.
This could result in some false negatives.
With some insight into the algorithm, as presented in \cref{dlm:inf:algsection}, we note that the effect of false negatives is that the labels inferred could be more relaxed than possibly needed.
The algorithm does however ensure that all information flow constraints are still valid.

We provide the following definitions given the above description:
\begin{itemize}
  \item $L_1 \sqsubseteq L_3 \wedge L_2 \sqsubseteq L_3 \Rightarrow L_1 \sqcup L_2 \sqsubseteq L_3$ (As described in \cref{dlm:comparison})
  \item $L_1 \sqsubseteq L_2 \vee L_1 \sqsubseteq L_3 \Rightarrow L_1 \sqsubseteq L_2 \sqcup L_3$
  \item $L_1 \sqsubseteq L_3 \vee L_2 \sqsubseteq L_3 \Rightarrow L_1 \sqcap L_2 \sqsubseteq L_3$
  \item $L_1 \sqsubseteq L_2 \wedge L_1 \sqsubseteq L_3 \Rightarrow L_1 \sqsubseteq L_2 \sqcap L_3$
\end{itemize}

These definitions for determining how restrictive labels are will be used for the inference algorithm.

\subsection{Output channels}
As previously described, the way output channels are checked is by comparing the reader set of the output with the effective reader set of the outputted value(s).
This means that for an output channel with readers $R_c$ and some labeled output values $V$, we must have that
\[ \forall v \in V.R_c \subseteq readers(\underline{v}) \]
in order for the information flow to be valid.

We will instead opt for a different method, so that we can leave the checking of output channels to our inference algorithm, similar to any other inferrable construct.
This is so that we can use inference for output channels as well, which is not possible when using the ``simple'' comparison of reader sets.

\begin{definition}{Output channel constraints}\label{dlm:def:outputconstraints}
Let $ \{p_1, p_2, \dots, p_k\} $ be the set of all principals in a system.
Then for an output channel with readers $R_c$ and labeled input values $V$, we define an associated label:

\[ L_c = \{ p_1 \rightarrow R_c; p_2 \rightarrow R_c, \dots, p_k \rightarrow R_c \} \]

That is, a label owned by all the principals in the system, where each owner allows all principals in $R_c$ to read.
With this label, we give the following definition for the output channel:

\[ \forall v \in V.\underline{v} \sqsubseteq L_c \]
\end{definition}

This approach will allow the inference algorithm to check the reader set of an output channel, effectively providing inference for any values that are passed to an output channel.
Below we demonstrate that this approach is equivalent to comparing reader sets, by showing that

\[ \underline{v} \sqsubseteq L_c \Rightarrow R_c \subseteq readers(\underline{v}) \text{, for any } v \in V \]

When evaluating the above constraint we note that \cref{dlm:def:restrict:owners} in \cref{dlm:def:restrict} will always be true, as $owners(L_c)$ is the set of all principals.
We will then apply \cref{dlm:def:restrict:readers} to the same constraint to get

\[ \forall o \in owners(\underline{v}) . readers(\underline{v}, o) \supseteq readers(L_c, o) \]

From \cref{dlm:def:outputconstraints} we know that $readers(L_c, o) = R_c$, for any principal $o$ in the system.
Finally we see from \cref{dlm:def:effectivereaders} that $readers(\underline{v}, o) \supseteq readers(\underline{v})$, for any principal $o$ in the system.
Thus we know that

\[ readers(\underline{v}, o) \supseteq readers(\underline{v}) \supseteq R_c \text{, for any } o \in owners(\underline{v}) \]

\subsection{Inference algorithm}\label{dlm:inf:algsection}
Using the established extensions of labels we present the inference algorithm (see \cref{dlm:inf:algorithm}).
In the following we provide an informal description of the algorithm to aid the reader.

Note that the input for the algorithm is a set of constraints.
In \cref{extraction} we detail how to extract all constraints from source code, though the input for the algorithm can be \textit{any} set of constraints (see \cref{dlm:constraints}).
The only exception from this rule is that $\sqcap$ labels are not allowed on the left side of a constraint.
Why this is the case will become apparent as we walk through the algorithm.
The algorithm does not return a result, but instead works through the side effect of updating the current upper bound for variable labels.

\input{algorithms/inference.tex}

\myparagraph{Initialization}
As described in \cref{dlm:inf:types}, the initial current upper bound of a variable label must be $\top$.
Thus for all constraints with a variable label on the left hand side, we set the current upper bound of that label.
We use $\text{cub}(L)$ to denote the current upper bound of a variable label $L$ during execution of the algorithm.

With this we define another function that takes a label as input and returns a new label where variables are replaced by their current upper bound.
This function will be applied to labels in the algorithm before performing comparisons of those labels.

\[
\text{novar}(L) =
\begin{cases}
  \text{cub}(L) & \text{ if } L \text{ is a variable label}\\
  \text{novar}(L_3) \sqcup \text{novar}(L_4) & \text{ if } L_1 = L_3 \sqcup L_4\\
  \text{novar}(L_3) \sqcap \text{novar}(L_4) & \text{ if } L_1 = L_3 \sqcap L_4\\
  L & \text{ otherwise}
\end{cases}
\]

It could be the case that a constraint is on the form $L_1 \sqcup L_2 \sqsubseteq L_3$.
If $L_1$ or $L_2$ is a variable label in such a constraint their current upper bound would not be set.
Thus we would want to translate the constraint such that there are no joins on the left hand side of it.
In \cref{dlm:comparison} we noted that $L_1 \sqcup L_2 \sqsubseteq L_3$ and $L_1 \sqsubseteq L_3 \wedge L_2 \sqsubseteq L_3$ are equivalent.

With this we define a function that takes a constraint as input and produces a set of constraints where the left hand side has no joined labels.
This function is applied to all constraints before setting the current upper bound of the variable labels.

\[
\text{unjoin}("L_1 \sqsubseteq L_2") =
\begin{cases}
  \text{unjoin}(L_3) \cup \text{unjoin}(L_4) & \text{ if } L_1 = L_3 \sqcup L_4\\
  \{"L_1 \sqsubseteq L_2"\} & \text{ otherwise}
\end{cases}
\]

Because meet labels are not allowed in label declarations, we can be sure that the left hand side of all constraints is either a set of policies, a variable label or a constant label.

\myparagraph{Computing labels}
Having taking the necessary initializing steps the algorithm checks that each constraint in the constraints set is valid.
If all constraints are valid we know that the information flow of the program is valid and the algorithm can terminate.
Thus, if we have explicitly stated all labels, the algorithm simply performs a check of weather the information flow is valid.

If a constraint is not valid one of two things will happen, based on the type of label on the left hand side of the constraint:
\begin{itemize}
  \item If the label is a variable, we update its current upper bound to \textit{include} the right hand side of the constraint.
  This is achieved by meeting the two labels.
  \item If the label is not a variable, we can do nothing to relax it and the inference will fail.
  This is the case we will end up in when considering a program with invalid information flow and only explicit or constant labels.
\end{itemize}

Overall the idea of the algorithm is to initialize all variable labels to $\top$ and then relax them until all constraints are valid.
It might be tempting to make a label stricter such that a specific constraint is valid.
This is however not an option, as labels are only relaxed exactly enough that constraints are valid.
Making a label stricter would thus break a previously checked label.

% Describe why inference is so nice.
% Describe the different types of labels.
% Present the inference algorithm.
